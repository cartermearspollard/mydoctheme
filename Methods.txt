Methods
        Byron White is notoriously difficult to ideologically categorize. For this reason, I thought it would be interesting to look into the language and apply Jonathan Haidt’s Moral Foundations Theory to see if the language indicated anything about his White’s political preferences. The Moral Foundations Theory proposes that there are six moral foundations upon which all other values are based: care/harm, fairness/cheating, loyalty/betrayal, sanctity/degradation, authority/subversion, and liberty/oppression. In his book, The Righteous Mind, Haidt proposes that liberals rely primarily on the care/harm, fairness/cheating, and loyalty/betrayal foundations while conservatives value all six moral foundations. I found that Haidt often applied the theory to language used by different political parties, and my inquiry arose out of the question of whether it could be applied to such politically ambiguous language as White’s opinions.
        I collected 17 opinions, listed in the bibliography, and attempted to analyze their language based on this mode of questioning. After all files had been downloaded and converted to plain text, I began thinking about how I could analyze the data. Coincidentally, I had signed up for a short introduction to Python workshop offered by the library, and the very basic skills I had learned during that workshop provided me with a place to start using Python for data analysis. I began by using the code Dr. Thompson generously shared with me, but I found that it did not address the types of data I was looking for. I then used a Programming Historian tutorial on tf-idf, or Term Frequency-Inverse Document Frequency, to find the most used words from each case. In retrospect, I find that my error arises from analyzing each document separately rather than as a corpus. The most frequent words were very indicative of what each case was about, but they told me little about the moral language within the cases. However, at the time, I thought the difficulty arose from a lack of appropriate stop words. So, I created my own stop word list that was tailored to the language of the law. This helped, but it still did not yield the results I was looking for.
        At this point, I moved back to Dr. Thompson’s code and used sentiment analysis to see if there was anything notable to be found. Again, it did not yield any results I was looking for. The language of the law is purposefully neutral, if not stern, and because of this, I did not find that sentiment analysis yielded useful results. However, using her code, I did create a word cloud which enabled me to see that there was interesting language within my corpus worth pursuing. Dr. Wiscomb encouraged me to use Word2Vec and Stanford’s Named Entity Recognition (NER) program to look more closely at the data from a different viewpoint. Although I could not manage to use Word2Vec in Python due to my own technical skill limitations, I was able to use Stanza, a Python package developed by Stanford’s Named Language Processing Group, to extract all named entities within the corpus. NER extracts all persons, locations, and organizations mentioned within a document, and I found that this was rather overwhelming as Supreme Court opinions so often reference these various entities. I did not find anything of particular note within this data, but this can also be attributed to my removal of footnotes when I cleaned the data. As I was unable to use Word2Vec in Python, I instead employed topic modeling. Although the two are, in retrospect, highly dissimilar, I thought topic models might provide something of similar use to the data Word2Vec would have yielded.
        Using Melanie Walsh’s Introduction to Cultural Analytics & Python topic modeling tutorial, I downloaded MALLET and used Little MALLET Wrapper in Python to create my topic models. I found that these results were useful, particularly in regards to cases that were grouped together in the topic models. I then graphed the probability distributions for 15 topics according to which cases were most likely to appear in each topic. Although all this data was more than enough to work with, I decided to take one more step and use part of speech tagging for my corpus. Again, I used a tutorial from Walsh and applied it to my corpus. This yielded by far the most interesting results. I found that White rarely used complex language, particularly in his verbs. In this way, my findings were similar to “Elements of Judicial Style,” a study of Neil Gorsuch’s opinions conducted by Nina Varsava at Stanford.
        In addition to the text analysis I conducted in Python, I also found that data from the Supreme Court Database which catalogs various aspects of each case heard by the Supreme Court, including the votes of each justice, the issue area of each case, and the direction that each decision moves the Court left or right on the political spectrum. I collected the totals for the majority, dissent, and concurring votes of Byron White, Thurgood Marshall, Antonin Scalia, Warren Burger, William Rehnquist, Sandra Day O’Connor, John Harlan II, Potter Stewart, and Earl Warren, all of whom served on the Supreme Court with Justice White. This data is particularly interesting in the context it provides for the length of White’s tenure on the Court, 31 years, and the proportion of various votes in regards to other justices known to be either liberal or conservative.
        I found that displaying this type of data was difficult to do in an engaging way. In an attempt to make it more visually appealing, I used the Programming Historian tutorial on data visualization using Bokeh and Pandas in Python to create graphs that I felt represented interesting aspects of the data. Bokeh was particularly useful for its hover tool which displays additional data not represented in the graph, such as the words or total vote counts. Although the graphs are not as sophisticated as they could be, the process of learning how to display data in an engaging manner was useful in thinking about how I can best display this type of information. Moving forward, I think this study could be widened to analyze various justices, as demonstrated in “Elements of Judicial Style.” Furthermore, the language of politics in regards to political affiliation has become increasingly apparent in its power. The judicial branch of government holds enormous amounts of power and has often faded into the background. Justices are traditionally non-partisan, but as this is no longer the case, a study of their language holds the potential to reveal political affiliations that are otherwise difficult to recognize in official opinions. If this study were to be expanded upon, a cross referencing of the language used to by politicians and justices could yield insight into the rhetoric of politics and the law that would otherwise be obscured.